#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import gzip
import os
import re
import tarfile
import tweet_crawler as tc

from tensorflow.python.platform import gfile
from six.moves import urllib

import time
from datetime import datetime as dt, timedelta
import shutil


_PAD = "_PAD"
_GO = "_GO"
_EOS = "_EOS"
_UNK = "_UNK"
_START_VOCAB = [_PAD, _GO, _EOS, _UNK]

PAD_ID = 0
GO_ID = 1
EOS_ID = 2
UNK_ID = 3

_WORD_SPLIT = re.compile("([.,!?\"':;)(])")
_DIGIT_RE = re.compile(r"\d")


def basic_tokenizer(sentence):
    words = []
    for space_separated_fragment in sentence.strip().split():
        words.extend(re.split(_WORD_SPLIT, space_separated_fragment))
    # é…åˆ—wordsã‚’å±•é–‹ã—ãŸw("w for w in words")ã®ã†ã¡ã€ãƒ–ãƒ©ãƒ³ã‚¯ã§ãªã‹ã£ãŸw(if w)ã‚’è¿”å´ã™ã‚‹é…åˆ—ã«å…¥ã‚Œã¦è¿”ã™
    return [w for w in words if w]


def create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,
                      tokenizer=None, normalize_digits=True):
    if os.path.exists(vocabulary_path):
        print("Creating vocabulary %s from data %s" % (vocabulary_path, data_path))
        vocab = {}

        f = open(data_path,"r")
        counter = 0
        for line in f:
            counter = counter + 1
            if counter % 100 == 0:
                print("  processing line %d" % counter)
            tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)
            for w in tokens:
                word = re.sub(_DIGIT_RE, "0", w) if normalize_digits else w
                if word in vocab:
                    vocab[word] += 1
                else:
                    vocab[word] = 1
        vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)
        if len(vocab_list) > max_vocabulary_size:
            vocab_list = vocab_list[:max_vocabulary_size]

        vocab_file = open(vocabulary_path,"w")
        for w in vocab_list:
            vocab_file.write(w + "\n")
        vocab_file.close()
        f.close()

def create_vocabularyByArray(vocabulary_path, data_path, text_Array, max_vocabulary_size,
                             tokenizer=None, normalize_digits=True):

    if not os.path.exists(vocabulary_path):
        return

    print("Creating vocabulary %s from Twitter_data " % (vocabulary_path))
    vocab = {}

    f = open(data_path,"w")
    counterA = 0
    p = re.compile("([A-Za-z0-9\'~+\-=_.,/%\?!;:@#\*&\(\)ã€Šã€‹â—‡â–¼â—â‡’ğŸ’•âœ¨â˜ºâ™¥â‰ªğŸ˜‚ğŸ’“â‘ â™ªğŸ˜€ ğŸ˜¬ ğŸ˜ ğŸ˜‚ ğŸ˜ƒ ğŸ˜„ ğŸ˜… ğŸ˜† ğŸ˜‡ ğŸ˜‰ ğŸ˜Š ğŸ™‚ ğŸ™ƒ â˜ºï¸ ğŸ˜‹ ğŸ˜Œ ğŸ˜ ğŸ˜˜ ğŸ˜— ğŸ˜™ ğŸ˜š ğŸ˜œ ğŸ˜ ğŸ˜› ğŸ¤‘ ğŸ¤“ ğŸ˜ ğŸ¤— ğŸ˜ ğŸ˜¶ ğŸ˜ ğŸ˜‘ ğŸ˜’ ğŸ™„ ğŸ¤” ğŸ˜³ ğŸ˜ ğŸ˜Ÿ ğŸ˜  ğŸ˜¡ ğŸ˜” ğŸ˜• ğŸ™ â˜¹ï¸ ğŸ˜£ ğŸ˜– ğŸ˜« ğŸ˜© ğŸ˜¤ ğŸ˜® ğŸ˜± ğŸ˜¨ ğŸ˜° ğŸ˜¯ ğŸ˜¦ ğŸ˜§ ğŸ˜¢ ğŸ˜¥ ğŸ˜ª ğŸ˜“ ğŸ˜­ ğŸ˜µ ğŸ˜² ğŸ¤ ğŸ˜· ğŸ¤’ ğŸ¤• ğŸ˜´ ğŸ’¤ ğŸ’© ğŸ˜ˆ ğŸ‘¿ ğŸ‘¹ ğŸ‘º ğŸ’€ ğŸ‘» ğŸ‘½ ğŸ¤– ğŸ˜º ğŸ˜¸ ğŸ˜¹ ğŸ˜» ğŸ˜¼ ğŸ˜½ ğŸ™€ ğŸ˜¿ ğŸ˜¾ ğŸ™Œ ğŸ‘ ğŸ‘‹ ğŸ‘ ğŸ‘ ğŸ‘Š âœŠ âœŒï¸ ğŸ‘Œ âœ‹ ğŸ‘ ğŸ’ª ğŸ™ â˜ï¸ ğŸ‘† ğŸ‘‡ ğŸ‘ˆ ğŸ‘‰ ğŸ–• ğŸ– ğŸ¤˜ ğŸ–– âœï¸ ğŸ’… ğŸ‘„ ğŸ‘… ğŸ‘‚ ğŸ‘ƒ ğŸ‘ ğŸ‘€ ğŸ‘¤ ğŸ‘¥ ğŸ—£ ğŸ‘¶ ğŸ‘¦ ğŸ‘§ ğŸ‘¨ ğŸ‘© ğŸ‘± ğŸ‘´ ğŸ‘µ ğŸ‘² ğŸ‘³ ğŸ‘® ğŸ‘· ğŸ’‚ ğŸ•µ ğŸ… ğŸ‘¼ ğŸ‘¸ ğŸ‘° ğŸš¶ ğŸƒ ğŸ’ƒ ğŸ‘¯ ğŸ‘« ğŸ‘¬ ğŸ‘­ ğŸ™‡ ğŸ’ ğŸ™… ğŸ™† ğŸ™‹ ğŸ™ ğŸ™ ğŸ’‡ ğŸ’† ğŸ’‘ ğŸ‘©â€â¤ï¸â€ğŸ‘© ğŸ‘¨â€â¤ï¸â€ğŸ‘¨ ğŸ’ ğŸ‘©â€â¤ï¸â€ğŸ’‹â€ğŸ‘© ğŸ‘¨â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ ğŸ‘ª ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘§ ğŸ‘©â€ğŸ‘©â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘§ ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘§ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘¨â€ğŸ‘¨â€ğŸ‘§â€ğŸ‘§ ğŸ‘š ğŸ‘• ğŸ‘– ğŸ‘” ğŸ‘— ğŸ‘™ ğŸ‘˜ ğŸ’„ ğŸ’‹ ğŸ‘£ ğŸ‘  ğŸ‘¡ ğŸ‘¢ ğŸ‘ ğŸ‘Ÿ ğŸ‘’ ğŸ© â›‘ ğŸ“ ğŸ‘‘ ğŸ’ ğŸ‘ ğŸ‘› ğŸ‘œ ğŸ’¼ ğŸ‘“ ğŸ•¶ğŸ ğŸ ğŸ ğŸ¶ ğŸ± ğŸ­ ğŸ¹ ğŸ° ğŸ» ğŸ¼ ğŸ¨ ğŸ¯ ğŸ¦ ğŸ® ğŸ· ğŸ½ ğŸ¸ ğŸ™ ğŸµ ğŸ™ˆ ğŸ™‰ ğŸ™Š ğŸ’ ğŸ” ğŸ§ ğŸ¦ ğŸ¤ ğŸ£ ğŸ¥ ğŸº ğŸ— ğŸ´ ğŸ¦„ ğŸ ğŸ› ğŸŒ ğŸ ğŸœ ğŸ•· ğŸ¦‚ ğŸ¦€ ğŸ ğŸ¢ ğŸ  ğŸŸ ğŸ¡ ğŸ¬ ğŸ³ ğŸ‹ ğŸŠ ğŸ† ğŸ… ğŸƒ ğŸ‚ ğŸ„ ğŸª ğŸ« ğŸ˜ ğŸ ğŸ ğŸ‘ ğŸ ğŸ– ğŸ€ ğŸ ğŸ“ ğŸ¦ƒ ğŸ•Š ğŸ• ğŸ© ğŸˆ ğŸ‡ ğŸ¿ ğŸ¾ ğŸ‰ ğŸ² ğŸŒµ ğŸ„ ğŸŒ² ğŸŒ³ ğŸŒ´ ğŸŒ± ğŸŒ¿ â˜˜ ğŸ€ ğŸ ğŸ‹ ğŸƒ ğŸ‚ ğŸ ğŸŒ¾ ğŸŒº ğŸŒ» ğŸŒ¹ ğŸŒ· ğŸŒ¼ ğŸŒ¸ ğŸ’ ğŸ„ ğŸŒ° ğŸƒ ğŸš ğŸ•¸ ğŸŒ ğŸŒ ğŸŒ ğŸŒ• ğŸŒ– ğŸŒ— ğŸŒ˜ ğŸŒ‘ ğŸŒ’ ğŸŒ“ ğŸŒ” ğŸŒš ğŸŒ ğŸŒ› ğŸŒœ ğŸŒ ğŸŒ™ â­ï¸ ğŸŒŸ ğŸ’« âœ¨ â˜„ï¸ â˜€ï¸ ğŸŒ¤ â›…ï¸ ğŸŒ¥ ğŸŒ¦ â˜ï¸ ğŸŒ§ â›ˆ ğŸŒ© âš¡ï¸ ğŸ”¥ ğŸ’¥ â„ï¸ ğŸŒ¨ â˜ƒï¸ â›„ï¸ ğŸŒ¬ ğŸ’¨ ğŸŒª ğŸŒ« â˜‚ï¸ â˜”ï¸ ğŸ’§ ğŸ’¦ ğŸŒŠ ğŸ’ ğŸŒ‚ğŸŠ ğŸ‹ ğŸŒ ğŸ‰ ğŸ‡ ğŸ“ ğŸˆ ğŸ’ ğŸ‘ ğŸ ğŸ… ğŸ† ğŸŒ¶ ğŸŒ½ ğŸ  ğŸ¯ ğŸ ğŸ§€ ğŸ— ğŸ– ğŸ¤ ğŸ³ ğŸ” ğŸŸ ğŸŒ­ ğŸ• ğŸ ğŸŒ® ğŸŒ¯ ğŸœ ğŸ² ğŸ¥ ğŸ£ ğŸ± ğŸ› ğŸ™ ğŸš ğŸ˜ ğŸ¢ ğŸ¡ ğŸ§ ğŸ¨ ğŸ¦ ğŸ° ğŸ‚ ğŸ® ğŸ¬ ğŸ­ ğŸ« ğŸ¿ ğŸ© ğŸª ğŸº ğŸ» ğŸ· ğŸ¸ ğŸ¹ ğŸ¾ ğŸ¶ ğŸµ â˜•ï¸ ğŸ¼ ğŸ´ ğŸ½ ğŸ€ ğŸˆ âš¾ï¸ ğŸ¾ ğŸ ğŸ‰ ğŸ± â›³ï¸ ğŸŒ ğŸ“ ğŸ¸ ğŸ’ ğŸ‘ ğŸ ğŸ¿ â›· ğŸ‚ â›¸ ğŸ¹ ğŸ£ ğŸš£ ğŸŠ ğŸ„ ğŸ›€ â›¹ ğŸ‹ ğŸš´ ğŸšµ ğŸ‡ ğŸ•´ ğŸ† ğŸ½ ğŸ… ğŸ– ğŸ— ğŸµ ğŸ« ğŸŸ ğŸ­ ğŸ¨ ğŸª ğŸ¤ ğŸ§ ğŸ¼ ğŸ¹ ğŸ· ğŸº ğŸ¸ ğŸ» ğŸ¬ ğŸ® ğŸ‘¾ ğŸ¯ ğŸ² ğŸ° ğŸ³âš½ï¸ ğŸš— ğŸš• ğŸš™ ğŸšŒ ğŸš ğŸ ğŸš“ ğŸš‘ ğŸš’ ğŸš ğŸšš ğŸš› ğŸšœ ğŸ ğŸš² ğŸš¨ ğŸš” ğŸš ğŸš˜ ğŸš– ğŸš¡ ğŸš  ğŸšŸ ğŸšƒ ğŸš‹ ğŸš ğŸš„ ğŸš… ğŸšˆ ğŸš ğŸš‚ ğŸš† ğŸš‡ ğŸšŠ ğŸš‰ ğŸš ğŸ›© âœˆï¸ ğŸ›« ğŸ›¬ â›µï¸ ğŸ›¥ ğŸš¤ â›´ ğŸ›³ ğŸš€ ğŸ›° ğŸ’º âš“ï¸ ğŸš§ â›½ï¸ ğŸš ğŸš¦ ğŸš¥ ğŸ ğŸš¢ ğŸ¡ ğŸ¢ ğŸ  ğŸ— ğŸŒ ğŸ—¼ ğŸ­ â›²ï¸ ğŸ‘ â›° ğŸ” ğŸ—» ğŸŒ‹ ğŸ—¾ ğŸ• â›ºï¸ ğŸ ğŸ›£ ğŸ›¤ ğŸŒ… ğŸŒ„ ğŸœ ğŸ– ğŸ ğŸŒ‡ ğŸŒ† ğŸ™ ğŸŒƒ ğŸŒ‰ ğŸŒŒ ğŸŒ  ğŸ‡ ğŸ† ğŸŒˆ ğŸ˜ ğŸ° ğŸ¯ ğŸŸ ğŸ—½ ğŸ  ğŸ¡ ğŸš ğŸ¢ ğŸ¬ ğŸ£ ğŸ¤ ğŸ¥ ğŸ¦ ğŸ¨ ğŸª ğŸ« ğŸ© ğŸ’’ ğŸ› â›ªï¸ ğŸ•Œ ğŸ• ğŸ•‹ â›©â—†â‰«ğŸ™ğŸ˜ŠğŸ˜­â˜…âœ©â˜†â—†]+)")
    private_use = re.compile(ur'[\uE000-\uF8FF]')
    for line in text_Array:
        # print(type(line))
        counterA = counterA + 1
        if counterA % 100 == 0:
            print("  processing line %d" % counterA)
        tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)
        f.write(line + "\n")
        for w in tokens:
            if p.match(w) or private_use.match(w):
                print('match : ' + w)
                continue

            word = re.sub(_DIGIT_RE, "0", w) if normalize_digits else w
            if word in vocab:
                vocab[word] += 1
            else:
                vocab[word] = 1
    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)
    if len(vocab_list) > max_vocabulary_size:
        vocab_list = vocab_list[:max_vocabulary_size]

    vocab_file = open(vocabulary_path,"w")

    for w in vocab_list:
        print(w)
        # print(type(w))
        vocab_file.write(w + "\n")
    vocab_file.close()
    f.close()

def initialize_vocabulary(vocabulary_path):

    if not os.path.exists(vocabulary_path):
        # raise ValueError("Vocabulary file %s not found.", vocabulary_path)
        file = open(vocabulary_path,"w")
        file.close()

    rev_vocab = []

    f = open(vocabulary_path,"r")
    rev_vocab.extend(f.readlines())
    rev_vocab = [line.strip() for line in rev_vocab]
    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])
    f.close()
    return vocab, rev_vocab


def sentence_to_token_ids(sentence, vocabulary,
                          tokenizer=None, normalize_digits=True):

    if tokenizer:
        words = tokenizer(sentence)
    else:
        words = basic_tokenizer(sentence)
    if not normalize_digits:
        return [vocabulary.get(w, UNK_ID) for w in words]
    return [vocabulary.get(re.sub(_DIGIT_RE, "0", w), UNK_ID) for w in words]


def data_to_token_ids(data_path, target_path, vocabulary_path,
                      tokenizer=None, normalize_digits=True):

    print("Tokenizing data in %s" % data_path)
    vocab, _ = initialize_vocabulary(vocabulary_path)
    data_file = open(data_path,"r")
    tokens_file = open(target_path,"w")
    counter = 0
    for line in data_file:
        counter += 1
        if counter % 100 == 0:
            print("  tokenizing line %d" % counter)
        token_ids = sentence_to_token_ids(line, vocab, tokenizer,
                                          normalize_digits)
        tokens_file.write(" ".join([str(tok) for tok in token_ids]) + "\n")
    data_file.close()
    tokens_file.close()

def prepare_wmt_data(data_dir, in_vocabulary_size, out_vocabulary_size):



    train_path = os.path.join(data_dir, "train_data")
    dev_path = os.path.join(data_dir, "test_data")


    out_vocab_path = os.path.join(data_dir, "vocab_out.txt" )
    in_vocab_path = os.path.join(data_dir, "vocab_in.txt" )
    # create_vocabulary(out_vocab_path, train_path + "_out.txt", out_vocabulary_size)

    menu_file_path = os.path.join(data_dir, "searchTargets.txt")
    town_file_path = os.path.join(data_dir, "searchTowns.txt")

    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    shutil.copyfile(menu_file_path,os.path.join(data_dir, "searchTargetsBak.txt"))
    shutil.copyfile(town_file_path,os.path.join(data_dir, "searchTownsBak.txt"))

    menu_file = open(menu_file_path,"r+")
    town_file = open(town_file_path,"r+")
    tweetData = []
    towns = []
    targetMenu = []
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
    for line in menu_file:
        targetMenu.append(line)

    for line in town_file:
        towns.append(line)

    counter = 0
    # while True:
    isLimit = False
    # å‡¦ç†çµ‚äº†æ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ç”¨ã®æ¤œç´¢ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒªã‚¹ãƒˆ
    nextFileTarget = []

    # loopç”¨ãƒªã‚¹ãƒˆ
    # åˆæœŸå€¤ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚“ã ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€‚ä»¥é™ã¯ã€æ–°ã—ã„åè©ã‚’ã²ãŸã™ã‚‰è¿½åŠ â†’ãƒ«ãƒ¼ãƒ—ã™ã‚‹
    loopTarget = []
    loopTarget.extend(targetMenu)

    # æ¬¡ã®loopç”¨ãƒªã‚¹ãƒˆ
    # loopTargetã®ãƒ«ãƒ¼ãƒ—ã§ã€æ–°ã—ã„åè©ãŒè¦‹ã¤ã‹ã£ãŸæ™‚ã«ä¿æŒã—ã¦ãŠããƒªã‚¹ãƒˆã€‚ãã®ã¾ã¾loopTargetã«ä»£å…¥ã™ã‚‹
    nextLoopTarget = []

    # æ¬¡å›ä»¥é™æ¤œç´¢é™¤å¤–ç”¨ãƒªã‚¹ãƒˆ
    # æ¤œç´¢çµæœãŒ0ä»¶ã®å˜èª
    excludeWords = []

    # ã‚¿ã‚¦ãƒ³ã‚‚å¢—ã‚„ã—ã¦ã‚‚ã„ã„ã‹ã‚‚ã€‚èªå½™ã®å‹‰å¼·ã¨è€ƒãˆã‚Œã°è¿‘å ´ã§ãªãã¨ã‚‚ã€‚
    today = dt.now()
    # endTime = today + timedelta(hours=-1) # ãƒ«ãƒ¼ãƒ—ã—ãªã„
    # endTime = today + timedelta(hours=1)   # => 1æ™‚é–“å¾Œ
    # endTime = today + timedelta(hours=6)   # => 6æ™‚é–“å¾Œ
    # endTime = today + timedelta(hours=7)   # => 7æ™‚é–“å¾Œ
    endTime = today + timedelta(minutes=1)   # => 1åˆ†å¾Œ
    print(today)
    print(endTime)
    # æŒ‡å®šæ™‚é–“å›ã‚‹ãƒ«ãƒ¼ãƒ—
    while today <= endTime:
        # ç”ºãƒ«ãƒ¼ãƒ—
        for town in towns:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ«ãƒ¼ãƒ—
            for target in loopTarget:
                result, limit, reset, meishiList = tc.searchTweet(target + ' ' + town)
                # æ¤œç´¢çµæœãªã—ãªã®ã§æ¬¡ã®æ¤œç´¢ã¸
                if len(result) != 0:
                    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›å¯¾è±¡ã¸ã€æœªè¿½åŠ ãªã‚‰è¿½åŠ 
                    if not target in nextFileTarget:
                        nextFileTarget.append(target)

                    tweetData.extend(result)
                    for meishi in meishiList:
                        # å‡¦ç†çµ‚äº†æ™‚ã«ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã™ã‚‹å˜èªï¼ˆåè©ï¼‰ã®è¿½åŠ 
                        if not meishi in nextFileTarget and not meishi in targetMenu:
                            nextFileTarget.append(meishi)
                        # æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã§ä½¿ç”¨ã™ã‚‹åè©ã¨ã—ã¦è¿½åŠ 
                        if not meishi in loopTarget:
                            nextLoopTarget.append(meishi)

                # print('limit : ', limit)
                # æ¤œç´¢ãƒªãƒŸãƒƒãƒˆã«ãªã£ãŸå ´åˆã«ã¯ã€è§£é™¤ã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
                if limit == '0':
                    print('limit is zero.')
                    nowPre = dt.now()
                    if nowPre >= endTime:
                        break
                    epochPre = datetime_to_epoch(nowPre)
                    print('reset time is', epoch_to_datetime(float(reset)))
                    print('sleep time is', (float(reset) - epochPre + 1))
                    # isLimit = True
                    # åˆ¶é™è§£é™¤æ™‚é–“ãŒæ¥ã‚‹ã¾ã§å¾…æ©Ÿ
                    while True:
                        now = dt.now()
                        epoch = datetime_to_epoch(now)
                        if float(reset) <= epoch:
                            break
                        else:
                            time.sleep(float(reset) - epoch + 1)
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ«ãƒ¼ãƒ—ã“ã“ã¾ã§

            # æ™‚é–“ãŒããŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
            nowPre = dt.now()
            if nowPre >= endTime:
                break
            # ç”ºãƒ«ãƒ¼ãƒ—ã“ã“ã¾ã§


        # æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã§ä½¿ç”¨ã™ã‚‹æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ«ãƒ¼ãƒ—ç”¨ãƒªã‚¹ãƒˆã«ä»£å…¥ã™ã‚‹
        loopTarget = nextLoopTarget
        today = dt.now()
                #   break
            #       counter+=1
            #       if counter == 10:
            #         print('counter is 10')
            #         isLimit = True
            #         break
            # if isLimit:
            #   break
    # print(epoch_to_datetime(float(reset)))

    menu_file.close()
    town_file.close()
    # æ–°ã—ã„åè©ã‚’å«ã‚“ã ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã€‚å‰Šé™¤â†’æ–°è¦ã§ä½œã‚‹ãŒã€ã‚‚ã£ã¨ã„ã„æ–¹æ³•ã¯ãªã„ã‚‚ã®ã‹ã€‚ã€‚ã€‚
    os.remove(menu_file_path)
    menu_file = open(menu_file_path,"w")
    for line in nextFileTarget:
        menu_file.write(line + '\n')
    menu_file.close()

    create_vocabularyByArray(out_vocab_path, os.path.join(data_dir, "tweetDataFile_out.txt"), tweetData, out_vocabulary_size)
    # create_vocabulary(in_vocab_path, train_path + "_in.txt", in_vocabulary_size)
    create_vocabularyByArray(in_vocab_path, os.path.join(data_dir, "tweetDataFile_in.txt"), tweetData, in_vocabulary_size)

    out_train_ids_path = train_path + ("_ids_out.txt" )
    in_train_ids_path = train_path + ("_ids_in.txt" )
    data_to_token_ids(train_path + "_out.txt", out_train_ids_path, out_vocab_path)
    data_to_token_ids(train_path + "_in.txt", in_train_ids_path, in_vocab_path)


    out_dev_ids_path = dev_path + ("_ids_out.txt" )
    in_dev_ids_path = dev_path + ("_ids_in.txt" )
    data_to_token_ids(dev_path + "_out.txt", out_dev_ids_path, out_vocab_path)
    data_to_token_ids(dev_path + "_in.txt", in_dev_ids_path, in_vocab_path)

    return (in_train_ids_path, out_train_ids_path,
            in_dev_ids_path, out_dev_ids_path,
            in_vocab_path, out_vocab_path)

def epoch_to_datetime(epoch):
    return dt(*time.localtime(epoch)[:6])

def datetime_to_epoch(d):
    return int(time.mktime(d.timetuple()))
